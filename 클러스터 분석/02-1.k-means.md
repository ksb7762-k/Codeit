가장 먼저 k-means에 대해 알아보자  

## k-means 동작 방식  

k-means의 기본 아이디어는 다음과 같음  
- 유사한 데이터는 Centroid(중심점)로부터 가까이에 모여있다  

여기서 Centroid가 무엇을 의미할까? 그리고 얼마나 가까운 데이터들이 하나의 클러스터에 묶일까?  

하나씩 알아봄  

## 1단계: Centroid 배치  
먼저, 클러슽의 개수를 의미하는 k를 정해 줘야함.  예시로 2개 설정  
그리고 k의 값만큼 Centroid를 생성하여 임의로 배치  

## 2단계: 클러스터 형성  

생성한 Centroid와 각 데이터 사이의 거리를 계산하여 가까이에 있는 데이터들을 하나의 클러스터로 묶음  
   

![](/image.png/08.10.PNG)  


## 3단계: Centroid 위치 갱신  

클러스터에 속해있는 데이터들의 중심으로 Centroid의 위치를 이동.  이때, 데이터들 사이의 중심을 찾기 위해 평균값(means)사용   

![](/image.png/08.11.PNG)


## 4단계: 클러스터 재형성  

새롭게 위치한 Centroid를 기준으로 각 데이터와의 거리를 다시 계산하여 가까운 데이터들을 하나의 클러스터로 묶음  

![](/image.png/08.12.PNG) 

## 5단계: Centroid 위치 갱신  

새롭게 형성된 클러스터의 중심으로 Centroid를 다시 이동  

![](/image.png/08.13.PNG) 

실제 데이터로 해보면 이렇게 쉽게 되진 않음. 최적의 클러스터를 찾을 때까지 위읟 단계들을 여러 번 반복하면서 Centroid의 위치를 계속 옮겨 줘야 함  

그러다가 k개의 Centroid가 더 이상 위치를 갱신하지 않게 되면, 그 위치에서 가장 가까운 거리에 있는 데이터들이 하나의 클러스터가 됨  

이것이 k-means가 클러스터를 나누는 방식  
k를 몇으로 하냐에 따라 k-means 모델의 성능이 크게 달라짐  
적절한 클러스터 개수를 구하는 방법은 앞으로 배움  

## 최적의 k 선정 기준 (inertia) 

최적의 클러스터 개수는 어떻게 찾을 수 있을까?  그를 위해선 k-means가 잘 됐다는 것을 판단할 수 있는 기준이 필요  

k-means는 k 개의 Centroid에 가까이 모여 있는 데이터들을 하나의 클러스터로 묶어주는 방법이었음  
그렇다면, 클러스터마다 속한 데이터와 Centroid 사이 거리의 합이 작아야 잘 된 클러스터링임  

그걸 확인하기 위해 사용하는 값이 바로 intertia(이너시아)  
inertia는 각 클러스터에 속한 데이터들과 Centroid 사이의 거리를 제곱해서 전부 더한 값  

예시)  
7개의 데이터를 두 개의 클러스터로 나눠봄  

![](/image.png/08.14.PNG) 

위의 예시에서 intertia는 43이 나옴  

## Elbow Method  

그런데, k를 하나로만 정의해서 inertia를 구하면 해당 값이 큰 것인지 작은 것인지 판단이 어려움  
==> 그래서 서로 다른 k갑 여러개로 모델을 만든 다음에 각각의 inertia 값을 비교해 봐야함  

scaled_df라는 데이터셋에서 elbow mothod를 했다고 하자  
k의 개수를 1부터 15까지 다르게 하여 각각의 inertia 값을 계산하고, 해당 값들을 시각화해보자  

![](/image.png/08.15.PNG)  

k가 커질수록 intertia는 작아짐  
하지만 클러스터의 개수가 많아질수록 클러스터링을 하는 의미가 사라짐  

따라서 최적의 클러스터 개수는 intertia가 충분히 작지만, 분석 목적에 부합하도록 적당해야함  

그리고 보통 그 지점은 시각화 한 그래프의 기울기가 급격하게 줄어드는 구간으로 정의함   

위의 그래프를 보면 k값이 2~3 사이인 구간에서 기울기가 급격하게 줄어듬==> 따라서 여기서 최적의 값은 2나 3이라고 볼 수 있음  

이 떄 그래프의 모양이 마치 팔꿈치 모양 같다고 해서, 이런 식으로 클러스터 개수를 찾는 방법을 Elbow Method라고 함 

그런데 Elbow Method를 통해 나온 결과를 반드시 따라야 하는건 아님  
보통 보조 지표 정도로 활용하고,  상황이나 목적에 맞게 클러스터 개수를 조금 다르게 설정해도 괜찮음  


## k-means의 장단점  

장점 : k-means는 변수들에 대한 배경지식, 역할, 영향도에 대해 모르더라도 데이터 사이의 거리만 구할 수 있다면 쉽게 사용가능 
또 알고리즘이 비교적 쉬운 수식으로 이루어졌기 때문에 이해와 해석이 용이함  

단점: 최적의 클러스터 개수인 k를 정하는게 어려움  
앞서 배운 Elbow Method로 단서를 얻는 것은 가능하지만, 가이드일뿐 정답은 아님  

또, k-means는 이상치에 영향을 많이 받음. 이상치가 포함된 데이터일 경우 Centroid를 업데이트 하는 과정에서 Centroid의 위치가 크게 변동되고, 클러스터가 원하지 않는 방식으로 묶일 수 있음   

![](/image.png/08.16.PNG)   

![](/image.png/08.17.PNG) 

마지막으로, k=means는 초기 Centroid가 어떻게 설정되었는지에 따라 결과가 달라짐. 또, 적절하지 않은 곳에 배치되면 위치를 너무 많이 옮겨야 해서 연산이 오래 걸릴 수 있고 경우에 따라서는 특정한 한곳으로 수렴하지 못하는 경우도 발생  

이런 이유때문에 k-means++ 모델이 등장  

파이썬에서는 다음과 같이 구현  

from sklearn.cluster import KMeans  
model = KMeans(n_clusters=k, init='k-means++')  

이외에도 차원이 높은 데이터에 적용할 때 성능이 떨어진다는 단점이 있음 

해당 내용은 다음 레슨에서 자세히 다룸  


## 차원의 저주  

변수가 더 많아질수록 모델의 성능은 오히려 나빠질 수 있음.  
이런 현상을 차원이 저주라고 함 

먼저 차원이 증가하면 어떤 일이 일어나는지 부터 살펴보자.  

첫번째 그래프는 1000개의 데이터를 1차원 공간에 랜덤하게 배치한 것  
1차원 공간에서는 데이터들 간의 거리가 매우 좁고, 빈 공간이 적음  

![](/image.png/08.18.PNG) 

두 번째 그래프는 1000개의 데이터를 2차원 공간에 랜덤하게 배치. 첫 번쨰 그래프보다 데이터 간 거리가 넚고, 빈 공간도 많아짐  

![](/image.png/08.19.PNG) 

마지막으로 3차원 공간에 1000개의 데이터를 배치하면 어떻게 될까?  

![](/image.png/08.20.PNG)  

2차원 공간에 데이터를 배치했을 떄보다 빈 공간이 좀 더 많아짐  

데이터 간의 거리가 멀어질수록 클러스터링 진행 시 데이터 간의 유사성을 계산하는게 어렵고 정확도도 떨어짐  

특히, 거리 기반 모델인 k-means에서는 문제거 더 큼  
가까운 데이터와 먼 데이터의 구분이 어려워지기 떄문에 클러스터를 나누는 성능이 떨어질 수 있음. 때문에 변수가 너무 많은 데이터를 거리 기반 알고르짐으로 클러스터링 하면 문제가 될 수 있음  

