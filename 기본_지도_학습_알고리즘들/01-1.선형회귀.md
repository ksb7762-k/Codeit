# 선형 회귀 (Linear Regression)

집 가격 예측 머신러닝 프로그램  

문제를 단수환하기 ㅜ이해 오직 집크기만으로 집가격 예측  

우선 주어진 데이터만 보자     

이걸 그래프로 표현해보자  
  
우리는 이 집 데이터를 프로그램에게 학습시키려고 함  

그리고나서 이 프로그램이 새로운 집을 보았을때 크기만 가지고 가격을 예측하도록 하는 것  

어떻게 할 수 있을까?  
방법은 다양. 그중 가장 대표적인 알고르짐  

선형회귀 (Linear Regession)
데이터를 가장 잘 대변해주는 서능ㄹ 찾아내는것  통계학에서는 이 선을 최적선 (Line of Best fit)이라고 함   

최적선을 찾았다고 가정 그러면 이 선을 어떻게 활용할 수 있을까? 
만약 50평인 집의 가격을 알고 싶으면 이 선에서 칮으면 됨.   

선형 회귀는 단순하면서도 유용하고 다른 알고르짐의 기반이 됨.

## 선형 회귀 용어  

목표 변수 : 맞추려고 하는 값 (target variable / output variable)  
입력 변수 : 맞추는데 사용하는 값 (input variable/ feature)  

## 데이터 표현법 

우리가 프로그램을 학습시키기 위해 사용하는 데이터들을 '학습 데이터'라고 함.  
학습 데이터의 개수를 보통 m이라는 문자로 표현  


## 가설 함수   

우리는 최적선을 찾아내기 위해 다양한 함수를 시도함. 우리가 시도하는 이 함수 하나하나를 가설함수(hypothesis function)라고 함. 

가설함수는 h라는 문자로 표현 입력변수 x에 대한 함수, 그리고 Θ(세타)라는 그리스 문자를 사용 

실제로는 입력변수가 많기 떄문에 일관성있게   
$$
\theta_0,\ \theta_1,\ \theta_2,\ \dots,\ \theta_n
$$

 등오르 씀   

따라서 선형회귀는 가장 적절한 세타 값들을 찾아내는 것     

## 평균 제곱 오차 (MSE)  

입력 변수가 1개(집 크기)라고 가정  

$$
h_\theta(x) = \theta_0 + \theta_1 x
$$
  
그리고 그래프에서 3가지 가설함수를 살펴보자    
2번째가 낫다는걸 감이 아니라 기준을 두고 비교   

가설 함수 평가법  
평균 제곱 오차 (Mean Squaerd Error)  : MSE   
이 데이터들과 가설함수가 평균적으로 얼마나 떨어져 있는지 나타내는 방법 중 
하나  

오차값들을 모두 제곱하고 이 제곱한 값들을 더함 그리고 이것의 평균을 내기 위해 데이터 개수만큼 나누면 됨.

왜 오차의 제곱을 더할까? 1. 차이를 계산하면 양수 5든 마이너스 5든 똑같이 취급해야 함 ==> 모두 양수로 통일   
2. 더 큰 오차를 부각시킬 수 있음! (더 큰 오차에 대해서 더 큰 페널티를 주기 위함 )  


두 번쨰 가설함수의 MSE가 가장 작기 떄문에 두번쨰 가설함수가 이 데이터들과 가장 잘 맞는다고 볼 수 있습니다. 
 
 ## MSE의 일반화 

## 손실 함수 (Loss function)  혹은 비용 함수 (Cost function)  
: 가설함수의 성능을 평가하는 함수  
손할 함수가 작으면 : 가설함수가 데이터에 잘 맞다 

손실함수의 인풋은 세타  즉, 손싷함수의 아웃풋은 이 세타값들을 어ㄸ허게 설정하는냐에 달려있음  
x,y는 사실 정해진 데이터를 대입하는 것이기 떄문에 변수가 아니라 상수  

## 경사 하강법 (Gradient Descent)  

 선형회귀의 목적 ㅣ 데이터에 가장 잘맞는 가설함수 찾기   
세타 0과 세타 1을 조율하면서 최적선을 찾아야함   
가설함수를 최적선이 되게끔 개선하려면 가섫마수르 평가하는 기준이 있어야함 
그 기준이 바로 손실함수 
(세타 값들을 바꿔 손실 함수의 아웃풋을 최소화)  how?  
그 방법들 중 하나가 경사 하강법   

간단한 예시  
손실함수의 아웃풋을 최소화 즉 극소점을 찾고 싶음  
일단은 어디선가 시작을 해야하니까 세타값들을 모두 랜덤하게 지정하거나 모두 0으로 설정    

경사의 방향과 크기를 이용해서 극소점을 향해 내려가는 방식  

손실함수 j의 인풋이 2개라고 가정 
손실함수 j를 시각적으로 표현하면 다음과 같이 3차원 그래프가 그려짐. 
여기서도 마찬가지로 극소점을 향해 가야됨  
그리고 특정 지점에 있다고 가정하고 그 지점의 기울기를 구함   
j를 각 인풋변수에 대해서 편미분하고 그 식에 현재 위치의 좌표를 대입하고 벡터로 만들면 그 위치에서의 기울기를 구할 수 있음   

우리는 극소점을 찾고 싶기 떄문에 가장 가파르게 내려가는 방향으로 가야함   
이 기울기 벡터에 -붙이면 됨.  이 방향으로 움직이면 가설함수가 개선되어서 손실함수의 극소점에 가까워 질 것임 이 과정을 반복하면 결국에는 극소점을 찾을 수 있음 그리고 손실함수에 매우 가까워지면 가설함수가 최적하되었다고 볼 수 있음   

### 2편에서 계속... 

