머신 러닝 모델이 정확한 예측을 못하는 경우가 많음  
이번 챕터에서는 이런 문제를 해결하는 방법을 배움  

## 편향과 분산   

모델이 너무 간단해서 데이터의 관계를 학습하지 못하는 경우, 편향(bias)이 높다고 말함    

예시 

![](/image.png/05.8.PNG)  

이번에는 모델의 복잡도를 늘려서 (높은 차항의 회귀를 이용해서) 모델을 학습시켰다고 하자  

예시  

![](/image.png/05.9.PNG)    

그렇다면 편향이 낮은 모델이 높은 모델보다 항상 더 좋을까? ==>  그건 아님  

데이터셋 별로 모델이 얼마나 일관된 성능을 보여주는 지를 분산이라고 함  

성능 차이가 많이 나면 분산이 높다, 성능 차이가 없으면 분산이 낮다고 함 


## 편향-분산 트레이드오프(Bias-Variance Tradeoff)   

![](/image.png/05.10.PNG)       

![](/image.png/05.11.PNG)      

일반적으로 편향과 분산은 하나가 줄어들면 하나는 늘어나는 관계가 있음 ==> 이걸 편향-분산 트레이드오프라 부름   
적당한 밸런스를 찾아내는 게 중요! 

즉 아래와 같은 곡선을 찾아야함  

![](/image.png/05.12.PNG)    

## 정규화(Regularization) 개념  

모델이 과적합 돼서 다음과 같은 복잡한 다항함수가 나왔다고 하자   

![](/image.png/05.13.PNG)     

함수가 급격하게 변하는 이유는 세타값들이 크기 때문임   

![](/image.png/05.14.PNG)    


정규화는 모델을 학습시킬 때 세타값들이 너무 커지는 걸 방지해서 과적합을 예방   

트레이닝 데이터에 대한 오차값은 커질 수 있어도 가설함수를 완만하게 만들어서 여러 데이터 셋에서 일관된 성능을 보임    

![](/image.png/05.15.PNG)  
  
## L1, L2 정규화

머신러닝에서 정규화는 손실함수에 정규화 항을 더해서 세타값들이 커지는 것을 방지  


이게 무슨 뜻?     

 ![](/image.png/05.16.PNG)   

새로운 기준을 적용하여 새로운 손실함수를 만듬     

![](/image.png/05.17.PNG)  

두 개의 가설 함수를 비교해보자   

![](/image.png/05.18.PNG)  


결과적으로 g(x)가 더 좋은 가설함수라고 평가할 수 있음   
사실 정규화항에는 람다라는 상수도 곱해줌  

![](/image.png/05.19.PNG)   

이건 세타값들이 커지는 것에 대해서 얼마나 많은 페널티를 줄 것인지 정함   

![](/image.png/05.20.PNG)   


예를 들어, 람다가 100이면 세타 값들이 조금만 커져도 손실 함수가 굉장히 커지기 때문에 세타값을 줄이는 게 중요하고   

람다가 0.01이면 세타값들이 많이 커져도 손실 함수가 별로 안 커지기 때문에 평균 제곱 오차를 줄이는게 우선  

우리가 여태까지 배운 정규화 방식을 L1 정규화라고 함 
그리고 L1 정규화를 적용하는 회귀모델을 LASSO REGRESSION(Lasso 모델)이라고 함      

![](/image.png/05.21.PNG)   
 

L2 정규화도 개념은 똑같은데 
세타값의 절댓값이 아니라 제곱값을 더함    
  
L2 정규화를 적용한 모델을 Ridge Regression(Ridge 모델)이라고 함   
  
![](/image.png/05.22.PNG)    





