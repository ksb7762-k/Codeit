##  Boosting

bagging 복습 : 임의로 bootstrap 데이터 셋들을 만든다  
데이터 셋들을 사용해서 수만흥ㄴ 모델들을 만들고 이 모델들의 예측ㅇ르 종합함  

boosting : 앙상블 기법 중 하나 
일부러 성능이 안좋은 모델들을 사용함  
먼저 만든 모델들이 어떻게 예측을 했냐에 따라서 뒤에 만드는 모델들의 ㄷ이터셋들이 ㄷ정해짐  
결정을 내릴 때 단순 다수결이 아니라 성능이 더 좋은 모델들의 예측값들 더 많이 반영함 ㅇ

핵심은 성능이 안좋은 약한 학습자(weak learner)들을 합쳐서 성능을 극대화한다!  

결정트리 부슽이의 시초 알고리즘인 에다부스트에 대해서 알아보자  

다 부스트(Adaboost) 

## Adaboost 개요 

랜포를 만들떄와 똑같이 수많은 결정트리륾 만듬  
깊은 결정트리들이 아니라 투트노드 하나와 분류 노드 2개를 갖는 얕은 결정트리를 만듬  

교통사고 데잍터를 예시로 

07.35

이런 식으로 하나의 질문과 질문에 대한 다븡로 바로 예측을 한ㄴ 결정트리를 스텀프라고 함  
(stump)
스텀프는 평균적으로 50퍼센트보다 조금 좋은 성능을 가짐  
부스팅 기버븡ㄴ 성능이 안좋은 모델들을 사용 그래서 일반결정 트리가 아닌 
stump만을 사용 
그리고 각 모델이 사용하는 데이터셋을 임의로 만들지 않음 
이걸 어떻게 하띾> 

예시 ) 07.36
데이터가 있다고 하자 파란색과 발간색 데이터는서로 분류가 다른데 이걸 이요해서 결정 스텀프 하나를 마들었다고하자 그럼 이 겾어 스컴프가 맞게 분류하는 얘들이 잇고 틀리게 ㅣ분류하느 얘들이 있음  그럼 
다음 스텀프의 데이터셋을 만들때 앞에 있는 스텀프가 틀린 애들의 중요도를 높여줌 
맞체 예측한 데이터의 중요도를 낮춰준다 
중요도가 높은 얘들은 뒹 ㅔ만들 스텀프들이 더 우선적으로 맞출수 있게함 
중요도가 낮은 얟르은 덜 신경쓰게 해줌
  이렇게 또 
스텀프를 만듬 그다음 텀프를 만들떄도 데이터셋에서 예측에 틀린 데이터들과 맞은 데이터들의 주용도를 조절 전에 틀렸던 데이터들을 더 잘 예측하는 스텀프를 만듬  

이런 식으로 각 ㅅ텀프는 전에 있ㄴ던 스텀프ㄷ의 실수를 바로 잡는 방햐응로 만들어줌  

이걸 미리 정해놓은 만큼 반복해서 엄첨 많은 스텀프를 만듬  
07.37
이후 다수결의 원칙이 아니라 성능주의적 예측으로 함 
예를 들어 스텀프이 성능이 다음과 같이 있을때 이 4개의 스텀프가 최종결정에 다른 영향력을 가진다고 하자 

각 성능의 합이 높은 결정에 따름 
07.38 

정리 

07.39 

## 스텀프 성능 계산하기 

에다 부스트 알고리즘에 대해서 자세하게 알아보자 
데이터는 독감데이터로 이번에는 숫자 데이터도 추가 
시작하기전에 즁요도라는 열을 추가 
실제로 속성에 추갛나느 것은 아니고 스텀프를 만들떄는 전에 예측에 실패했던 데이터들을 좀더 중요하게 취급 ==> 이걸 수치화하고 보기 편하게 열로 추가시킴  
처음에는 틀리게 예측하는 데이터가 없으니까 몯ㄴ 데이터의 중요도를 같게 설정  
중요도의 합은 항상 1로 유지  

07.40    

첫 스텀프는 각 분류/질문들의 지니불순도를 계산해서 가장 낮은 노드를 정함 
열이 37.5도 이상인가요?로 정했다고 가정 
07.41

스ㅓㅁ프는 질문이 하나밖ㅇ ㅔ없의낙 넘으면 독감 아니면 일반 감기로 분류 
스텀프를 만들떄마다 성능을 계산헤야함 
스텀프의 성능은 다음과 같이 계산 


07.42

total_error : 잘못 분류한 데이터 중요도의 합   
37.5도를 기준으로 했을대 2개의 데이터가 틀림 

07.43 

이 데이터의 중요도들이 각각 1/7이니까 total error는 2/7  
식에 넣으면 스텀프의 성능은 0.458이 나옴  

07.44 

성능식을 그래프로 그려보자  
07.45

total_error가 1에 가까워 질수록 작아지고 0에 가까워 질수록 커짐  
total_eeor = 1일나ㅡㄴ것은 슽텀프가 모든 데이터를 ㅌ르리게 예측한 경우 
성능을 무한히 안좋게 만들어줌  

total_error = 0.5라는 것은 모든 중요도의 합중 딱 반만 맞은 거니까 성능이 아무 의미가 없어서 0으로 만들어줌  

정리  

07.46 

## 데이터 중요도 바꾸기  


트릴게 예측한 데이터의 중요ㄷ는 높여주고 맞게 예측한 데이터의 주요도는 줄여도는 방법을 보자 

먼저 틀리게 분류한 모든 데이터에 대해서는 중요도를 다음과 같이 바꿔줌  

07.47 

Ptree는 스텀프의 성능  

제대로 분류한 모든 데이터에 대해서는 중요도를 다음과 같이 계산  

07.48  

e의 Ptree제곱을 그래프로 표현하면 다음과 같다  

07.49  

데이터셋을 가지고 계산해보자 

일단 모든 데이터의 중요도가 1/7임 
그리고 이 스텀프의 선으은 0.458이라고 했느이가 각 데이터에 적용하면 새로운 중요도를 구할 수 있음 
07.50

틀린 데이터들의 중요도는 0.2226 맞게 분류한 데이터들의 중요돈ㄴ 0.09인걸 확인할 수 있음  
중요도의 합은 항상 1  
지금은 1보다 작으니까 비율적으로 조절을 해줘야함   

07.51  

조정하면  

07.52  

## 스텀프 추가하기  

머리에 과부하... ==> 흐름만 파악 

07.53   

결론은 뒤 스펌프들은 앞 스텀프들의 실수를 더 잘 맞추게 된다는 것 

## 에다 부스트 예측  

최종 예측을 하는 방법을 보자  

에다 부스트는 성능이 좋은 스텀프들에 가중치  
편의상 에다부스트를 이용해서 4개의 스텀프를 만들었다고 하자 

07.54 

07.55 



