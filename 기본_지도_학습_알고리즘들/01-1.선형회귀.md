# 선형 회귀 (Linear Regression)

예시) 집 가격 예측 머신러닝 프로그램

문제를 단순화해서 오직 *집 크기*만으로 집 가격 예측

우선 주어진 데이터만 보자  

![](/image.png/집가격.png)

이걸 그래프로 표현해보자
  


![](/image.png/집가격2.png)


우리는 이 집 데이터를 프로그램에게 학습시키려고 함

그리고 나서 이 프로그램이 새로운 집을 보았을 때 크기만 가지고 가격을 예측하도록 하는 것

어떻게 할 수 있을까

방법은 다양. 그중 가장 대표적인 알고리즘

선형 회귀 (Linear Regression)  
데이터를 가장 잘 대변해주는 선을 찾아내는 것. 통계학에서는 이 선을 최적선 (Line of Best Fit)이라고 함

최적선을 찾았다고 가정. 그러면 이 선을 어떻게 활용할 수 있을까  
만약 50평인 집의 가격을 알고 싶으면 이 선에서 찾으면 됨

선형 회귀는 단순하면서도 유용하고 다른 알고리즘의 기반이 됨

## 선형 회귀 용어

목표 변수 : 맞추려고 하는 값 (target variable / output variable)  
입력 변수 : 맞추는 데 사용하는 값 (input variable / feature)
  
![](/image.png/목표1.png)

## 데이터 표현법

우리가 프로그램을 학습시키기 위해 사용하는 데이터들을 '학습 데이터'라고 함  
학습 데이터의 개수를 보통 m이라는 문자로 표현

![](/image.png/데이터표현.png)

## 가설 함수

우리는 최적선을 찾아내기 위해 다양한 함수를 시도함. 우리가 시도하는 이 함수 하나하나를 가설 함수 (hypothesis function)라고 함

가설 함수는 h라는 문자로 표현. 입력 변수 x에 대한 함수, 그리고 Θ(세타)라는 그리스 문자를 사용

실제로는 입력 변수가 많기 때문에 일관성 있게 다음과 같이 씀  
  

  ![](/image.png/가설함수1.png)


따라서 선형 회귀는 가장 적절한 세타 값들을 찾아내는 것

## 평균 제곱 오차 (Mean Squared Error) : MSE

입력 변수가 1개 (집 크기)라고 가정

$$
h_\theta(x) = \theta_0 + \theta_1 x
$$

그리고 그래프에서 3가지 가설 함수를 살펴보자  
2번째가 낫다는 걸 감이 아니라 기준을 두고 비교하려면

![](/image.png/평가1.png)

## 평균 제곱 오차 (Mean Squared Error) : MSE

이 데이터들과 가설 함수가 평균적으로 얼마나 떨어져 있는지 나타내는 방법

오차 값들을 모두 제곱하고 이 제곱한 값들을 더함. 그리고 이것의 평균을 내기 위해 데이터 개수만큼 나누면 됨

$$
\text{MSE}(\theta) = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - f_\theta(x_i) \right)^2
$$

왜 오차의 제곱을 더할까    
1. 차이를 계산하면 +5든 -5든 똑같이 취급해야 함 → 모두 양수로 통일    
2. 더 큰 오차를 부각시킬 수 있음 (더 큰 오차에 대해서 더 큰 페널티를 주기 위함)

두 번째 가설 함수의 MSE가 가장 작기 때문에 두 번째 가설 함수가 이 데이터들과 가장 잘 맞는다고 볼 수 있음

## MSE의 일반화


![](/image.png/MSE일반화.png)



## 손실 함수 (Loss Function) 혹은 비용 함수 (Cost Function)

가설 함수의 성능을 평가하는 함수  
손실 함수가 작으면 → 가설 함수가 데이터에 잘 맞는 것  


![](/image.png/손실.png)


손실 함수의 인풋은 Θ. 즉, 손실 함수의 아웃풋은 이 Θ값들을 어떻게 설정하느냐에 달려 있음  
여기에서 x, y는 정해진 데이터를 대입하는 것이기 때문에 변수가 아니라 상수

## 경사 하강법 (Gradient Descent)

선형 회귀의 목적 : 데이터에 가장 잘 맞는 가설 함수를 찾기  
\( \theta_0 \)과 \( \theta_1 \)을 조율하면서 최적선을 찾아야 함  
가설 함수를 최적선이 되게끔 개선하려면 가설 함수를 평가하는 기준이 있어야 함  
그 기준이 바로 손실 함수 
Θ값들을 바꿔가며 손실 함수의 아웃풋을 최소화. 그 방법들 중 하나가 경사 하강법

간단한 예시  
손실 함수의 아웃풋을 최소화, 즉 극소점을 찾고 싶음  
일단은 어디선가 시작을 해야 하니까 Θ값들을 모두 랜덤하게 지정하거나 모두 0으로 설정

경사의 방향과 크기를 이용해서 극소점을 향해 내려가는 방식

![](/image.png/손실2.png)

손실 함수 j의 인풋이 2개라고 가정  
손실 함수 j를 시각적으로 표현하면 다음과 같이 3차원 그래프가 그려짐 


![](/image.png/경사1.png)



여기서도 마찬가지로 극소점을 향해 가야 됨  
그리고 특정 지점에 있다고 가정하고 그 지점의 기울기를 구함  
j를 각 인풋 변수에 대해서 편미분하고 그 식에 현재 위치의 좌표를 대입하고 벡터로 만들면 그 위치에서의 기울기를 구할 수 있음

우리는 극소점을 찾고 싶기 때문에 가장 가파르게 내려가는 방향으로 가야 함  
이 기울기 벡터에 -를 붙이면 됨. 이 방향으로 움직이면 가설 함수가 개선되어서 손실 함수의 극소점에 가까워질 것임. 이 과정을 반복하면 결국에는 극소점을 찾을 수 있음. 그리고 손실 함수에 매우 가까워지면 가설 함수가 최적화되었다고 볼 수 있음

![](/image.png/경사2.png) 


### 2편에서 계속
