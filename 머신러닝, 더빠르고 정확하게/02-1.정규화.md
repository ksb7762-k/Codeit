머신 러닝 모델이 정확한 예측을 못하는 경우가 많음  
이번 챕터에서는 이런 문제를 해결하는 방법을 배움  

## 편향과 분산   

모델이 너무 간단해서 데이터의 관계를 학습하지 못하는 경우, 편향(bias)이 높다고 말함    

예시 

![](/image.png/05.8.PNG)  

이번에는 모델의 복잡도를 늘려서 (높은 차항의 회귀를 이용해서) 모델을 학습시켰다고 하자  

예시  

![](/image.png/05.9.PNG)    

그렇다면 편향이 낮은 모델이 높은 모델보다 항상 더 좋을까? ==>  그건 아님  

데이터셋 별로 모델이 얼마나 일관된 성능을 보여주는 지를 분산이라고 함  

성능 차이가 많이 나면 분산이 높다, 성능 차이가 없으면 분산이 낮다고 함 


## 편향-분산 트레이드오프(Bias-Variance Tradeoff)   

![](/image.png/05.10.PNG)       

![](/image.png/05.11.PNG)      

일반적으로 편향과 분산은 하나가 줄어들면 하나는 늘어나는 관계가 있음 ==> 이걸 편향-분산 트레이드오프라 부름   
적당한 밸런스를 찾아내는 게 중요! 

즉 아래와 같은 곡선을 찾아야함  

![](/image.png/05.12.PNG)    

## 정규화(Regularization) 개념  

모델이 과적합 돼서 다음과 같은 복잡한 다항함수가 나왔다고 하자   

![](/image.png/05.13.PNG)     

함수가 급격하게 변하는 이유는 세타값들이 크기 때문임   

![](/image.png/05.14.PNG)    


정규화는 모델을 학습시킬 때 세타값들이 너무 커지는 걸 방지해서 과적합을 예방   

트레이닝 데이터에 대한 오차값은 커질 수 있어도 가설함수를 완만하게 만들어서 여러 데이터 셋에서 일관된 성능을 보임    

![](/image.png/05.15.PNG)  
  
## L1, L2 정규화

머신러닝에서 정규화는 손실함수에 정규화 항을 더해서 세타값들이 커지는 것을 방지  


이게 무슨 뜻?     

 ![](/image.png/05.16.PNG)   

새로운 기준을 적용하여 새로운 손실함수를 만듬     

![](/image.png/05.17.PNG)  

두 개의 가설 함수를 비교해보자   

![](/image.png/05.18.PNG)  


결과적으로 g(x)가 더 좋은 가설함수라고 평가할 수 있음   
사실 정규화항에는 람다라는 상수도 곱해줌  

![](/image.png/05.19.PNG)   

이건 세타값들이 커지는 것에 대해서 얼마나 많은 페널티를 줄 것인지 정함   

![](/image.png/05.20.PNG)   


예를 들어, 람다가 100이면 세타 값들이 조금만 커져도 손실 함수가 굉장히 커지기 때문에 세타값을 줄이는 게 중요하고   

람다가 0.01이면 세타값들이 많이 커져도 손실 함수가 별로 안 커지기 때문에 평균 제곱 오차를 줄이는게 우선  

우리가 여태까지 배운 정규화 방식을 L1 정규화라고 함 
그리고 L1 정규화를 적용하는 회귀모델을 LASSO REGRESSION(Lasso 모델)이라고 함      

![](/image.png/05.21.PNG)   
 

L2 정규화도 개념은 똑같은데 
세타값의 절댓값이 아니라 제곱값을 더함    
  
L2 정규화를 적용한 모델을 Ridge Regression(Ridge 모델)이라고 함   
  
![](/image.png/05.22.PNG)    

## L2, L2 정규화 일반화  

정규화는 모델의 파라미터(즉 학습을 통해 찾고자 하는 값들 - 회귀의 경우 세타)에 대한 손실함수를 최소화 하는 모든 알고리즘에 적용 가능  
따라서 다중 회귀, (다중) 다항 회귀, 로지스틱 회귀 모델 모두에 정규호롸를 적용가능  
그냥 손실함수에 정규화 항 더해주면 됨  


sklearn에서 모델링할 떄는 알아서 정규화 적용해주는 모델을 사용하면 됨  
다중 회귀 또는 다항 회귀 모델을 만들 때는 LinearRegression 대신 Lasso 또는 Ridge 모델 사용하면 됨    

그렇다면 LogisticRegression에 정규화를 적용하고 싶으면 어떻게 해야할까?   
로지스틱 모델은 사실 자동으로 L2 정규화를 적용함  

어떤 정규화 기법을 사용할지는 모델의 penalty라는 옵셔널 파라미터로 정해줄 수 있음  


'''LogisticRegression(penalty='none')  # 정규화 사용 안함
LogisticRegression(penalty='l1')  # L1 정규화 사용
LogisticRegression(penalty='l2')  # L2 정규화 사용
LogisticRegression()  # 위와 똑같음: L2 정규화 사용'''  

딥러닝 모델도 손실함수를 최소화 하는 알고리즘    
딥러닝 모델로 과적합되는 경우가 많기 때문에 정규화가 중요  


## L1, L2 정규화의 차이점    

L1 정규화는 여러 세타값들을 0으로 만들어줌. 즉, 모델에 중요하지 않다고 생각되는 속성들을 아예 없애줌   

L2 정규화는 세타값들을 0으로 만들기 보다는 조금씩 줄여줌

예시  

![](/image.png/05.23.PNG)     






