##  Boosting

bagging 복습 : 임의로 bootstrap 데이터셋들을 만듬  

데이터 셋들을 사용해서 수많은 모델들을 만들고 이 모델들의 예측을 종합함  

boosting : 앙상블 기법 중 하나  

일부러 성능이 안좋은 모델들을 사용함  

먼저 만든 모델들이 어떻게 예측을 했냐에 따라서 뒤에 만드는 모델들의 데이터셋이 정해짐  

결정을 내릴 때 단순 다수결이 아니라 성능이 더 좋은 모델들의 예측값들 더 많이 반영함  

==> 성능이 안좋은 약한 학습자(weak learner)들을 합쳐서 성능을 극대화한다!  

결정트리 부스팅의 시초 알고리즘인 에다 부스트(Adaboost)에 대해서 알아보자  




## Adaboost 개요 

랜포를 만들 때와 똑같이 수많은 결정트리를 만듬   

깊은 결정트리들이 아니라 루트 노드 하나와 분류 노드 2개를 갖는 얕은 결정트리를 만듬  

교통사고 데이터를 예시로 보자  

![](/image.png/07.35.PNG)  


이런 식으로 하나의 질문과 질문에 대한 답으로 바로 예측을 하는 결정트리를 스텀프(stump)라고 함  

스텀프는 평균적으로 50퍼센트보다 조금 좋은 성능을 가짐  

부스팅 기법은 성능이 안좋은 모델들을 사용 그래서 일반결정 트리가 아닌 
stump만을 사용  

그리고 각 모델이 사용하는 데이터셋을 임의로 만들지 않음  

이걸 어떻게 할까? 

예시 )

![](/image.png/07.36.PNG)  

어떤 데이터셋이 있다고 하자  

 파란색과 빨간색 데이터는 서로 분류가 다른데 이걸 이용해서 결정 스텀프 하나를 만들었다고 하자  

이 결정 스텀프가 맞게 분류하는 얘들이 있고 틀리게 분류하는 얘들이 있을 것임  

   그럼 
다음 스텀프의 데이터셋을 만들 때 앞에 있는 스텀프가 틀린 얘들의 중요도를 높여줌 반대로 맞체 예측한 데이터의 중요도는 낮춤  

중요도가 높은 얘들은 뒤에 만드는 스텀프들이 더 우선적으로 맞출수 있게함  반대로 중요도가 낮은 얘들은 덜 신경쓰게 해줌  

다시  또 스텀프를 만듬.  그다음 스텀프를 만들 떄도 데이터셋에서 예측에 틀린 데이터들과 맞은 데이터들의 중요도를 조절해서 전에 틀렸던 데이터들을 더 잘 예측하는 스텀프를 만듬  


이런 식으로 각 스텀프는 전에 있던 스텀프의 실수를 바로 잡는 방향으로 만들어줌  

이걸 미리 정해놓은 만큼 반복해서 엄첨 많은 스텀프를 만듬 


![](/image.png/07.37.PNG)  


이후 다수결의 원칙이 아니라 성능주의적인 예측을 함  

예를 들어 스텀프이 성능이 다음과 같이 있을때 때 이 4개의 스텀프가 최종결정에 다른 영향력을 가진다고 하자 

각 성능의 합이 높은 결정에 따름  

![](/image.png/07.38.PNG)  



### 정리하자면   


![](/image.png/07.39.PNG)  



## 스텀프 성능 계산하기 

에다 부스트 알고리즘에 대해서 자세하게 알아보자  

데이터는 독감데이터로 이번에는 숫자 데이터도 추가!  

시작하기전에 즁요도라는 열을 추가  

실제로 속성에 추가하는 것은 아니고 스텀프를 만들 때는 전에 예측에 실패했던 데이터들을 좀더 중요하게 취급   
==> 이걸 수치화하고 보기 편하게 열로 추가시킴  

처음에는 틀리게 예측하는 데이터가 없으니까 모든 데이터의 중요도를 같게 설정  

중요도의 합은 항상 1로 유지!  


![](/image.png/07.40.PNG)  

   

첫 스텀프는 각 분류/질문들의 지니 불순도를 계산해서 가장 낮은 노드를 정함  

열이 37.5도 이상인가요?로 정했다고 가정  

![](/image.png/07.41.PNG)  



스텀프는 질문이 하나밖에 없으니까 넣으면 독감 아니면 일반 감기로 분류  

스텀프를 만들 때마다 성능을 계산헤야함   

스텀프의 성능은 다음과 같이 계산  



![](/image.png/07.42.PNG)  



total_error : 잘못 분류한 데이터 중요도의 합  

37.5도를 기준으로 했을 때 2개의 데이터가 틀림 

![](/image.png/07.43.PNG)  

 
이 데이터의 중요도들이 각각 1/7이니까 total error는 2/7  
식에 넣으면 스텀프의 성능은 0.458이 나옴  



![](/image.png/07.44.PNG)  


성능식을 그래프로 그려보자  

![](/image.png/07.45.PNG)  


total_error가 1에 가까워 질수록 작아지고 0에 가까워 질수록 커짐  

total_eeor = 1이라는 것은 스텀프가 모든 데이터를 틀리게 예측한 경우로
성능을 무한히 안좋게 만들어줌  

total_error = 0.5라는 것은 모든 중요도의 합 중 딱 반만 맞은 거니까 성능이 아무 의미가 없어서 0으로 만들어줌   

## 요약하자면 

![](/image.png/07.46.PNG)  


## 데이터 중요도 바꾸기   


툴라게 예측한 데이터의 중요도는 높여주고 맞게 예측한 데이터의 중요도는 줄여주는 방법을 알아보자   

먼저 틀리게 분류한 모든 데이터에 대해서는 중요도를 다음과 같이 바꿔줌  

![](/image.png/07.47.PNG)  


Ptree는 스텀프의 성능을 의미  


제대로 분류한 모든 데이터에 대해서는 중요도를 다음과 같이 계산  


![](/image.png/07.48.PNG)  


e의 Ptree제곱을 그래프로 표현하면 다음과 같다  


![](/image.png/07.49.PNG)  


데이터셋을 가지고 계산해보자  


일단 모든 데이터의 중요도가 1/7임  

그리고 이 스텀프의 성능은 0.458이니까 각 데이터에 적용하면 새로운 중요도를 구할 수 있음  

![](/image.png/07.50.PNG)  


틀린 데이터들의 중요도는 0.226 맞게 분류한 데이터들의 중요도는 0.09인걸 확인할 수 있음  

중요도의 합은 항상 1  

지금은 1보다 작으니까 비율조정을 해줘야함


![](/image.png/07.51.PNG)  



조정하면  

![](/image.png/07.52.PNG)  


## 스텀프 추가하기  

머리에 과부하... ==> 흐름만 파악 

![](/image.png/07.53.PNG)  


결론은 뒤 스펌프들은 앞 스텀프들의 실수를 더 잘 맞추게 된다는 것  


## 에다 부스트 예측  

최종 예측을 하는 방법을 보자  


에다 부스트는 성능이 좋은 스텀프들에 가중치  

편의상 에다부스트를 이용해서 4개의 스텀프를 만들었다고 하자  


![](/image.png/07.54.PNG)  


![](/image.png/07.55.PNG)  




